<<<<<<< HEAD
pdf_text <- pdf_text(file)
# Search for the word "hello" in the character vector
if (any(grepl("China", pdf_text))) {
# If the word is found, add the file to the results vector
results <- c(results, file)
}
}
files <- list.files(pattern = "*.pdf")
# Create an empty vector to store the names of the PDF files that contain the word
results <- c()
# Loop through the PDF files
for (file in files) {
# Read the PDF file
pdf_text <- pdf_text(file)
# Search for the word "hello" in the character vector
if (any(grepl("China", pdf_text))) {
# If the word is found, add the file to the results vector
results <- c(results, file)
}
}
print(results)
getOption("timeout")
options(timeout = 99999)
getOption("timeout")
library(tidyverse)
library(rvest)
# Read in the URL list from a CSV file
url_list <- read.csv("fara/blank-fara.csv")
getwd
getwd()
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/")
getwd()
library(tidyverse)
library(rvest)
# Read in the URL list from a CSV file
url_list <- read.csv("fara/blank-fara.csv")
# Loop through each URL and download the PDF
for (i in 1:nrow(url_list)) {
url <- url_list[i, "url"]
filename <- url_list[i, "filename"]
# Specify the path to the subfolder where you want to save the files
filepath <- paste0("fara/registrations/pdfs/blanks/", filename)
download.file(url, destfile = filepath, mode = "wb")
# Take a 10 second break every 5 requests
if (i %% 5 == 0) {
Sys.sleep(10)
=======
>>>>>>> main
}
}
getwd
getwd()
setwd("fara/registrations/blanks")
getwd()
setwd("fara/registrations/blanks/")
setwd("fara/registrations/pdfs/blanks/")
getwd()
# This first script will list the pdfs that contain the word "China"
files <- list.files(pattern = "*.pdf")
# Create an empty vector to store the names of the PDF files that contain the word
results <- c()
# Loop through the PDF files
for (file in files) {
# Read the PDF file
pdf_text <- pdf_text(file)
# Search for the word "hello" in the character vector
if (any(grepl("China", pdf_text))) {
# If the word is found, add the file to the results vector
results <- c(results, file)
}
}
# This first script will list the pdfs that contain the word "China"
library(pdftools)
files <- list.files(pattern = "*.pdf")
# Create an empty vector to store the names of the PDF files that contain the word
results <- c()
# Loop through the PDF files
for (file in files) {
# Read the PDF file
pdf_text <- pdf_text(file)
# Search for the word "hello" in the character vector
if (any(grepl("China", pdf_text))) {
# If the word is found, add the file to the results vector
results <- c(results, file)
}
}
head(results)
write.csv(results)
write.csv(results, "results.csv")
read.csv("results.csv")
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Use the file.rename() function to move the file
file.rename(from = file_name, to = "china-hit" + file_name)
}
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Use the file.rename() function to move the file
file.rename(from = file_name, to = "china-hit" + file_name)
}
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
warnings()
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
warnings()
getwd()
# Load the necessary libraries
library(tidyverse)
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
warnings()
getwd()
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("/china-hit/", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Concatenate the strings using the paste() function
to <- paste("china-hit", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name, sep="")
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name, sep="")
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
warnings()
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- "C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks/china-hit" + file_name
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
getwd()
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = 'file_name', to = to)
}
warnings()
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", 'file_name')
# Use the file.rename() function to move the file
file.rename(from = 'file_name', to = to)
}
warnings()
# Load the necessary libraries
library(tidyverse)
setwd("C:/Users/jakeh/Documents/Coding/rhetoric-xi/fara/registrations/pdfs/blanks")
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = 'file_name', to = to)
}
# Read in the CSV file
results <- read_csv("results.csv")
# Loop through each file in the list
for (i in 1:nrow(results)) {
# Get the file name from the list
file_name <- results$file_names[i]
# Concatenate the strings using the paste() function
to <- paste0(getwd(), "/china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
}
# Concatenate the strings using the paste() function
to <- paste("china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
# Concatenate the strings using the paste() function
to <- paste0("china-hit/", file_name)
# Use the file.rename() function to move the file
file.rename(from = file_name, to = to)
getwd()
require(readtext)
require(quanteda)
library(rtweet)
library(tidyverse)
library(pdftools)
library(quanteda.textplots)
# This runs the readtext command against all of Xi Jinping's translated speeches
xi_speeches <- readtext("fara/registrations/pdfs/china/*.pdf")
# This converts the readtext output into a corpus so that it can be analyzed using various quantdata packages
corpus_xi <- corpus(xi_speeches)
# Now that the corpus is created in R, it can be converted to tokens for various types of analysis.
# The following examples include different ways of creating tokens from the corpus, including by words, sentences, and characters.
doc.tokens <- tokens(corpus_xi)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
# This converts everything to lower case
doc.tokens <- tokens_tolower(doc.tokens)
kwic(doc.tokens, "china", window = 10)
fara_context <- kwic(doc.tokens, "china", window = 10)
write.csv(fara_context, "fara.context.csv")
# Set the API endpoint URL
url <- "https://efile.fara.gov/api/v1/RegDocs/csv/7057"
# Make the GET request
response <- GET(url)
library(httr)
# Set the API endpoint URL
url <- "https://efile.fara.gov/api/v1/RegDocs/csv/7057"
# Make the GET request
response <- GET(url)
head(response)
writeLines(response_data, "response6328.csv")
# Check the status code of the response
status_code <- status_code(response)
# If the status code is 200, the request was successful
if (status_code == 200) {
# Process the response
response_data <- content(response, as = "text")
print(response_data)
} else {
# There was an error with the request
print(paste("Request failed with status code", status_code))
}
head(response)
writeLines(response_data, "response6328.csv")
install.packages("plyr")
install.packages("stringr")
library(plyr)
library(stringr)
install.packages("stringr")
setwd("fara/registrations/pdfs/china")
getwd()
files <- list.files(pattern = "*.pdf")
head(files)
rename(files, str_replace(files, "china-hit", ""))
library(plyr)
library(stringr)
rename(files, str_replace(files, "china-hit", ""))
library(plyr)
library(stringr)
setwd("fara/registrations/pdfs/china")
getwd()
files <- list.files(pattern = "*.pdf")
head(files)
rename(files, str_replace(files, "china-hit", ""))
library(plyr)
library(stringr)
setwd("fara/registrations/pdfs/china/bad")
setwd("fara/registrations/pdfs/china/bad/")
setwd("bad")
files <- list.files(pattern = "*.pdf")
rename(files, str_replace(files, "china-hit", ""))
str_detect(files, "china-hit")
getwd()
rename(files, str_replace(files, "china-hit", ""))
files <- list.files
new_names <- gsub("^china-hit", "", files)
new_names <- gsub("china-hit", "", files)
files <- list.files()
new_names <- gsub("china-hit", "", files)
# Rename the files
for (i in 1:length(files)) {
file.rename(files[i], new_names[i])
}
source("~/Coding/rhetoric-xi/txt/foreignpolicy-speeches.R")
library(rvest)
library(readr)
df <- read_csv("selected2.csv")
# Initialize an empty character vector to store the extracted content
content <- character()
# Iterate through the rows of the data frame
for (i in 1:nrow(df)) {
# Read in the HTML of the webpage
html <- read_html(df$url[i])
# Extract the content of the elements with the "content" class
page_content <- html_node(html, ".content")
page_title <- html_nodes(html, ".title")
# Extract the text content of the element(s)
page_text <- html_text(page_content)
page_title_text <- html_text(page_title)
df <- data.frame(page_title_text, page_text)
# Append the extracted text to the content vector
content <- c(content, df)
}
library(rvest)
library(readr)
df <- read_csv("selected2.csv")
# Initialize an empty data frame to store the extracted content
content <- data.frame(page_title_text = character(), page_text = character())
# Iterate through the rows of the data frame
for (i in 1:nrow(df)) {
# Read in the HTML of the webpage
html <- read_html(df$url[i])
# Extract the content of the elements with the "content" class
page_content <- tryCatch(html_node(html, ".content"), error = function(e) NA)
page_title <- tryCatch(html_nodes(html, ".title"), error = function(e) NA)
# Extract the text content of the element(s)
page_text <- tryCatch(html_text(page_content), error = function(e) NA)
page_title_text <- tryCatch(html_text(page_title), error = function(e) NA)
# Append the extracted data to the content data frame
new_row <- data.frame(page_title_text, page_text)
content <- rbind(content, new_row)
}
source("~/Coding/rhetoric-xi/txt/foreignpolicy-speeches.R")
library(rvest)
library(readr)
df <- read_csv("selected2.csv")
# Initialize an empty data frame to store the extracted content
content <- data.frame(page_title_text = character(), page_text = character())
# Iterate through the rows of the data frame
for (i in 1:nrow(df)) {
# Read in the HTML of the webpage
html <- read_html(df$url[i])
# Extract the content of the elements with the "content" class
page_content <- tryCatch(html_node(html, ".content"), error = function(e) NA)
page_title <- tryCatch(html_nodes(html, ".title"), error = function(e) NA)
# Extract the text content of the element(s)
page_text <- tryCatch(html_text(page_content), error = function(e) NA)
page_title_text <- tryCatch(html_text(page_title), error = function(e) NA)
# Append the extracted data to the content data frame
new_row <- data.frame(page_title_text, page_text)
content <- rbind(content, new_row)
}
library(rvest)
library(readr)
df <- read_csv("selected2.csv")
# Initialize an empty data frame to store the extracted content
content <- data.frame(page_title_text = character(), page_text = character())
# Iterate through the rows of the data frame
for (i in 1:nrow(df)) {
# Read in the HTML of the webpage
html <- read_html(df$url[i])
# Extract the content of the elements with the "content" class
page_content <- tryCatch(html_node(html, ".content"), error = function(e) NA)
page_title <- tryCatch(html_nodes(html, ".title"), error = function(e) NA)
# Extract the text content of the element(s)
page_text <- tryCatch(unlist(lapply(page_content, html_text)), error = function(e) NA)
page_title_text <- tryCatch(unlist(lapply(page_title, html_text)), error = function(e) NA)
# Check if the extracted data is not empty
if (!is.null(page_title_text) && !is.null(page_text)) {
# Append the extracted data to the content data frame
new_row <- data.frame(page_title_text, page_text)
content <- rbind(content, new_row)
}
}
# Write the content data frame to a CSV file
write.csv(content, "output.csv")
source("~/Coding/rhetoric-xi/tweeps/add-twitter-acct.R")
library(rtweet)
library(rtweet)
library(dplyr)
library(readr)
auth <- rtweet_app()
userid <- readline("Enter a screen name: ")
folder_path <- file.path("tweeps/accts")
timeline <- get_timeline(userid, n = 50000, include_rts = TRUE)
# Get the expanded URLs for each tweet
urls <- expand_urls(timeline$text)
source("~/Coding/rhetoric-xi/tweeps/tweet-url-acct.R")
library(rtweet)
library(dplyr)
library(readr)
bits <- get_timeline("zlj517", n = 3200, include_rts = false)
bits <- get_timeline("zlj517", n = 3200, include_rts = FALSE)
head(bits)
write.csv(bits, "bits.csv")
df <- data.frame(bits)
write.csv(df, "bits.csv")
df <- as.data.frame(bits)
print(df)
write.csv(df, "bits.csv")
write_csv(df, "bits.csv")
<<<<<<< HEAD
source("~/Documents/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Documents/Coding/rhetoric-xi/fara/get-fara.R")
x <- c(0.5, 0.6)
x
x <- c(0.5, 0.6, "a")
x
x <- 0:6
x
as.logical(x)
as.character(x)
=======
source("~/Coding/rhetoric-xi/tweeps/viral.R")
head(tweets)
write_csv(tweets, "tweets.csv")
write.csv(tweets, "tweets.csv")
source("~/Coding/rhetoric-xi/tweeps/viral.R")
source("~/Coding/rhetoric-xi/tweeps/viral.R")
source("~/Coding/rhetoric-xi/tweeps/viral.R")
talkie <- readtext("output.txt")
library(rvest)
library(readr)
library(dplyr)
library(quanteda)
library(readtext)
talkie <- readtext("output.txt")
corpus_talkie <- corpus(talkie)
doc.tokens <- tokens(corpus_talkie)
doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_tolower(doc.tokens)
# Read in the stopwords from a CSV file
extrawords <- read.csv("extra-stop.csv", stringsAsFactors = FALSE)
extrawords <- unlist(extrawords)
doc.tokens <- tokens_remove(doc.tokens, pattern = extrawords, valuetype = 'fixed')
doc.dfm.xi <- dfm(doc.tokens)
junk <- topfeatures(doc.dfm.xi, 50)
write.csv(junk, "junk6.csv")
library(quanteda.textplots)
textplot_wordcloud(doc.dfm.xi)
textplot_wordcloud(doc.dfm.xi, min_count = 100)
textplot_wordcloud(doc.dfm.xi, min_count = 100, color = TRUE)
textplot_wordcloud(doc.dfm.xi, min_count = 100, color = "darkblue)
textplot_wordcloud(doc.dfm.xi, min_count = 100, color = "darkblue)
textplot_wordcloud(doc.dfm.xi, min_count = 100, color = "darkblue")
textplot_wordcloud(doc.dfm.xi, min_count = 100, color = "darkblue", ordered_color = TRUE)
textplot_wordcloud(doc.dfm.xi, min_count = 150)
textplot_wordcloud(doc.dfm.xi, min_count = 75)
textplot_wordcloud(doc.dfm.xi, min_count = 91)
# Read in the stopwords from a CSV file
extrawords <- read.csv("extra-stop.csv", stringsAsFactors = FALSE)
extrawords <- unlist(extrawords)
doc.tokens <- tokens_remove(doc.tokens, pattern = extrawords, valuetype = 'fixed')
doc.dfm.xi <- dfm(doc.tokens)
textplot_wordcloud(doc.dfm.xi, min_count = 91)
install.packages("wordcloud")
library(wordcloud)
wordcloud(doc.dfm.xi)
textplot_wordcloud(doc.dfm.xi, min_count = 91)
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
library(httr)
GET https://lda.senate.gov/api/v1/
GET("https://lda.senate.gov/api/v1/")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
source("~/Coding/rhetoric-xi/fara/get-fara.R")
>>>>>>> main
